{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ca380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# Configurações para melhor visualização\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Adiciona o caminho da pasta 'src' ao sistema\n",
    "# (../ sobe um nível, da pasta 'notebooks' para a raiz 'tech_challenge_voos')\n",
    "sys.path.append('../') \n",
    "\n",
    "# Importa nossa função customizada de carregamento\n",
    "from src.data_loader import load_datasets\n",
    "from src.preprocessing import clean_flights \n",
    "from src.feature_engineering import create_target_variable\n",
    "from src.modeling import create_preprocessing_pipeline\n",
    "\n",
    "print(\"Bibliotecas e módulos importados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados usando a função do script .py\n",
    "# (O caminho '../data/' sobe um nível e entra na pasta 'data')\n",
    "df_flights, df_airlines, df_airports = load_datasets('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f90eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora sim, a primeira análise!\n",
    "if df_flights is not None:\n",
    "    print(\"--- Informações de df_flights ---\")\n",
    "    df_flights.info()\n",
    "else:\n",
    "    print(\"Erro: DataFrame df_flights não foi carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Verificando valores ausentes\n",
    "if df_flights is not None:\n",
    "    print(\"Contagem de valores ausentes (nulos) por coluna:\")\n",
    "    print(df_flights.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Aplicando a limpeza\n",
    "if df_flights is not None:\n",
    "    print(f\"Tamanho original: {df_flights.shape}\")\n",
    "    \n",
    "    df_flights_clean = clean_flights(df_flights)\n",
    "    \n",
    "    print(f\"Tamanho limpo:    {df_flights_clean.shape}\")\n",
    "else:\n",
    "    print(\"df_flights não carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Verificação final de nulos\n",
    "if 'df_flights_clean' in locals():\n",
    "    print(\"\\nVerificando nulos no dataset limpo:\")\n",
    "    # .sum().sum() soma todos os nulos de todas as colunas\n",
    "    print(df_flights_clean.isnull().sum().sum())\n",
    "else:\n",
    "    print(\"df_flights_clean não foi criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Estatísticas Descritivas (Numéricas)\n",
    "if 'df_flights_clean' in locals():\n",
    "    print(\"Estatísticas Descritivas (Dados Numéricos):\")\n",
    "    display(df_flights_clean.describe())\n",
    "else:\n",
    "    print(\"df_flights_clean não foi criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Visualizando a distribuição dos atrasos na chegada\n",
    "if 'df_flights_clean' in locals():\n",
    "    print(\"Plotando a distribuição dos Atrasos na Chegada (ARRIVAL_DELAY)\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Usamos o `df_flights_clean`\n",
    "    sns.histplot(df_flights_clean['ARRIVAL_DELAY'], bins=100, kde=True)\n",
    "    \n",
    "    # Linha vertical vermelha para a média (4.4 min)\n",
    "    plt.axvline(\n",
    "        df_flights_clean['ARRIVAL_DELAY'].mean(), \n",
    "        color='red', \n",
    "        linestyle='--', \n",
    "        linewidth=2, \n",
    "        label=f'Média ({df_flights_clean[\"ARRIVAL_DELAY\"].mean():.2f})'\n",
    "    )\n",
    "    \n",
    "    # Linha vertical verde para a mediana (-5.0 min)\n",
    "    plt.axvline(\n",
    "        df_flights_clean['ARRIVAL_DELAY'].median(), \n",
    "        color='green', \n",
    "        linestyle='-', \n",
    "        linewidth=2, \n",
    "        label=f'Mediana ({df_flights_clean[\"ARRIVAL_DELAY\"].median():.2f})'\n",
    "    )\n",
    "    \n",
    "    # Dando \"zoom\" em uma área de interesse\n",
    "    # (Sem isso, o gráfico ficaria ilegível por causa dos outliers)\n",
    "    plt.xlim(-60, 180) \n",
    "    \n",
    "    plt.title('Distribuição dos Atrasos na Chegada (Zoom de -60 a 180 min)', fontsize=16)\n",
    "    plt.xlabel('Atraso na Chegada (Minutos)')\n",
    "    plt.ylabel('Contagem de Voos')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"df_flights_clean não foi criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 9: Criando a Variável Alvo (Target)\n",
    "if 'df_flights_clean' in locals():\n",
    "    \n",
    "    # Usamos a função do nosso script .py\n",
    "    df_model_data = create_target_variable(df_flights_clean, delay_threshold=15)\n",
    "    \n",
    "    print(\"\\nVerificando a distribuição da nova coluna 'IS_DELAYED':\")\n",
    "    # Mostra a contagem (0 = Não Atrasado, 1 = Atrasado) e a porcentagem\n",
    "    display(df_model_data['IS_DELAYED'].value_counts(normalize=True))\n",
    "    \n",
    "else:\n",
    "    print(\"df_flights_clean não foi criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba399f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10 (CORRIGIDA): Importações para Modelagem\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Esta linha diz ao Python para \"subir um nível\" (../) \n",
    "# e procurar por módulos lá. É o que permite encontrar a pasta 'src'.\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Agora estas importações vão funcionar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.modeling import create_preprocessing_pipeline\n",
    "from src.feature_engineering import engineer_time_features\n",
    "\n",
    "print(\"Importações de modelagem carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dded05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- CÉLULA 1: Criar as colunas de Engenharia de Feature ---\n",
    "\n",
    "try:\n",
    "    print(\"Criando 'HOUR_OF_DAY' e 'PART_OF_DAY'...\")\n",
    "    \n",
    "    # 1. Criar a coluna 'HOUR_OF_DAY'\n",
    "    # // 100 faz a divisão inteira (ex: 1150 -> 11). \n",
    "    # % 24 garante que a hora '24' (meia-noite) vire '0'.\n",
    "    df_flights_clean['HOUR_OF_DAY'] = (df_flights_clean['SCHEDULED_DEPARTURE'] // 100) % 24\n",
    "\n",
    "    # 2. Definir os \"cortes\" (bins) e os \"rótulos\" (labels)\n",
    "    # Bins:   -1 a 5 (Madrugada), 6 a 11 (Manhã), 12 a 17 (Tarde), 18 a 23 (Noite)\n",
    "    bins = [-1, 5, 11, 17, 23]\n",
    "    labels = ['Madrugada', 'Manhã', 'Tarde', 'Noite']\n",
    "\n",
    "    # 3. Criar a coluna 'PART_OF_DAY'\n",
    "    df_flights_clean['PART_OF_DAY'] = pd.cut(\n",
    "        df_flights_clean['HOUR_OF_DAY'], \n",
    "        bins=bins, \n",
    "        labels=labels, \n",
    "        right=True\n",
    "    )\n",
    "    \n",
    "    print(\"-> Colunas 'HOUR_OF_DAY' e 'PART_OF_DAY' criadas com sucesso!\")\n",
    "    print(\"\\n--- PREPARAÇÃO DO EIXO X CONCLUÍDA ---\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"Erro: A coluna 'SCHEDULED_DEPARTURE' não foi encontrada.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao criar PART_OF_DAY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0237c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Célula 10.5: Preparação Final para Modelagem ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Criar Features de Engenharia ---\n",
    "    # (Este é o código que estávamos depurando)\n",
    "    print(\"Criando 'HOUR_OF_DAY' e 'PART_OF_DAY'...\")\n",
    "    \n",
    "    # Garante que a coluna de hora está no formato certo\n",
    "    df_flights_clean['SCHEDULED_DEPARTURE'] = df_flights_clean['SCHEDULED_DEPARTURE'].astype(int)\n",
    "    \n",
    "    # // 100 faz a divisão inteira (ex: 1150 -> 11). \n",
    "    df_flights_clean['HOUR_OF_DAY'] = (df_flights_clean['SCHEDULED_DEPARTURE'] // 100) % 24\n",
    "    \n",
    "    # Define os \"cortes\" (bins) e os \"rótulos\" (labels)\n",
    "    bins = [-1, 5, 11, 17, 23]\n",
    "    labels = ['Madrugada', 'Manhã', 'Tarde', 'Noite']\n",
    "\n",
    "    # Cria a coluna 'PART_OF_DAY'\n",
    "    df_flights_clean['PART_OF_DAY'] = pd.cut(\n",
    "        df_flights_clean['HOUR_OF_DAY'], \n",
    "        bins=bins, \n",
    "        labels=labels, \n",
    "        right=True\n",
    "    )\n",
    "    print(\"-> Features de engenharia criadas.\")\n",
    "\n",
    "    # --- 2. Criar a Variável Alvo (Target) ---\n",
    "    print(\"Criando a variável alvo 'IS_DELAYED'...\")\n",
    "    LIMITE_ATRASO = 15 \n",
    "    \n",
    "    # (Vou usar 'IS_DELAYED' pois vi esse nome no seu código da Célula 11)\n",
    "    df_flights_clean['IS_DELAYED'] = (df_flights_clean['ARRIVAL_DELAY'] > LIMITE_ATRASO).astype(int)\n",
    "    print(\"-> Variável alvo 'IS_DELAYED' criada.\")\n",
    "\n",
    "    # --- 3. Definir as colunas FINAIS ---\n",
    "    # Esta é a lista das 9 features + 1 target que o Modelo 11 espera\n",
    "    FINAL_COLUMNS = [\n",
    "        # Numéricas\n",
    "        'MONTH', \n",
    "        'DAY_OF_WEEK', \n",
    "        'SCHEDULED_TIME', \n",
    "        'DISTANCE',\n",
    "        'HOUR_OF_DAY',\n",
    "        # Categóricas\n",
    "        'AIRLINE', \n",
    "        'ORIGIN_AIRPORT', \n",
    "        'DESTINATION_AIRPORT',\n",
    "        'PART_OF_DAY',\n",
    "        # Alvo\n",
    "        'IS_DELAYED' \n",
    "    ]\n",
    "    \n",
    "    # --- 4. Criar o DataFrame 'df_model_data_v2' ---\n",
    "    # Seleciona apenas as colunas que vamos usar e remove Nulos\n",
    "    # (O .dropna() aqui é uma segurança final, caso alguma feature nova tenha nulos)\n",
    "    df_model_data_v2 = df_flights_clean[FINAL_COLUMNS].dropna()\n",
    "\n",
    "    print(\"\\n--- SUCESSO! ---\")\n",
    "    print(\"DataFrame 'df_model_data_v2' criado com sucesso.\")\n",
    "    print(f\"Formato (shape) do dataset de modelagem: {df_model_data_v2.shape}\")\n",
    "    display(df_model_data_v2.head())\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERRO ---\")\n",
    "    print(f\"Erro de Chave (KeyError): A coluna {e} não foi encontrada.\")\n",
    "    print(\"Certifique-se de que 'df_flights_clean' existe e tem as colunas originais (ex: 'SCHEDULED_DEPARTURE', 'ARRIVAL_DELAY').\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CÉLULA 1: Criar a coluna-alvo (target) ---\n",
    "\n",
    "# O desafio pede para prever se um voo vai atrasar.\n",
    "# Vamos usar o padrão da indústria: um voo \"atrasado\" é aquele que chega\n",
    "# 15 minutos ou mais após o horário programado.\n",
    "LIMITE_ATRASO = 15 \n",
    "\n",
    "try:\n",
    "    # Cria a nova coluna. (df['col'] > X) vira True/False.\n",
    "    # .astype(int) transforma True/False em 1/0, o que é perfeito para cálculos.\n",
    "    df_flights_clean['ATRASOU_MAIS_15MIN'] = (df_flights_clean['ARRIVAL_DELAY'] > LIMITE_ATRASO).astype(int)\n",
    "    \n",
    "    print(\"Coluna 'ATRASOU_MAIS_15MIN' criada com sucesso!\")\n",
    "    \n",
    "    # Vamos verificar a proporção (é um dataset desbalanceado?)\n",
    "    # .value_counts(normalize=True) mostra a porcentagem\n",
    "    print(\"\\nProporção de voos Atrasados (1) vs. Não Atrasados (0):\")\n",
    "    print(df_flights_clean['ATRASOU_MAIS_15MIN'].value_counts(normalize=True))\n",
    "\n",
    "except KeyError:\n",
    "    print(\"Erro: A coluna 'ARRIVAL_DELAY' não foi encontrada. Verifique os passos anteriores.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CÉLULA 2: Gerar o gráfico (Agora vai funcionar) ---\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a ordem que você quer no eixo X\n",
    "ordem_partes_dia = ['Madrugada', 'Manhã', 'Tarde', 'Noite']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# O sns.barplot, por padrão, calcula a MÉDIA de 'y' para cada 'x'\n",
    "# Como 'y' é 0 ou 1, a média (ex: 0.22) é o mesmo que o percentual (22%)!\n",
    "sns.barplot(\n",
    "    x=\"PART_OF_DAY\",\n",
    "    y=\"ATRASOU_MAIS_15MIN\", \n",
    "    data=df_flights_clean,\n",
    "    order=ordem_partes_dia,\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "plt.title('Percentual de Voos com Atraso (>15min) por Período do Dia')\n",
    "plt.ylabel('Taxa de Atraso (Média de 0s e 1s)')\n",
    "plt.xlabel('Período do Dia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b79780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 11 (MODIFICADA - VERSÃO CORRETA COM 9 FEATURES)\n",
    "\n",
    "if 'df_model_data_v2' in locals():\n",
    "    \n",
    "    # 1. Definir NOVAS colunas de features (X)\n",
    "    numeric_features = [\n",
    "        'MONTH', \n",
    "        'DAY_OF_WEEK', \n",
    "        'SCHEDULED_TIME', \n",
    "        'DISTANCE',\n",
    "        'HOUR_OF_DAY'  # <-- Feature 5\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'AIRLINE',             # Feature 6\n",
    "        'ORIGIN_AIRPORT',      # Feature 7\n",
    "        'DESTINATION_AIRPORT', # Feature 8\n",
    "        'PART_OF_DAY'          # <-- Feature 9\n",
    "    ]\n",
    "    \n",
    "    TARGET = 'IS_DELAYED'\n",
    "    \n",
    "    # Garantir que as categóricas são strings (para o encoder)\n",
    "    df_model_ready = df_model_data_v2.copy()\n",
    "    for col in categorical_features:\n",
    "        df_model_ready[col] = df_model_ready[col].astype(str)\n",
    "            \n",
    "    # 2. Definir X e y a partir dos dados v2\n",
    "    FEATURES = numeric_features + categorical_features\n",
    "    X = df_model_ready[FEATURES]\n",
    "    y = df_model_ready[TARGET]\n",
    "    \n",
    "    # 3. Dividir os dados\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(\"Dados divididos em Treino e Teste (com Novas Features):\")\n",
    "    print(f\"X_train shape: {X_train.shape}\") # <-- DEVE MOSTRAR (..., 9)\n",
    "    print(f\"X_test shape:  {X_test.shape}\") # <-- DEVE MOSTRAR (..., 9)\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: df_model_data_v2 não foi criado. Rode a célula 'Nova Célula' (C-10.5) primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59337804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 12: Criando o pipeline de pré-processamento\n",
    "if 'numeric_features' in locals() and 'categorical_features' in locals():\n",
    "    \n",
    "    # Usamos a função do nosso script .py\n",
    "    preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
    "    \n",
    "    print(\"\\nDefinição do Pré-processador:\")\n",
    "    display(preprocessor)\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: Listas 'numeric_features' ou 'categorical_features' não definidas.\")\n",
    "    print(\"Por favor, rode a Célula 11 primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 13: Criar e Treinar o Pipeline (Baseline - LogisticRegression)\n",
    "\n",
    "# 1. Importações necessárias\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time # Para marcar o tempo de treino\n",
    "\n",
    "# 2. Criar o Pipeline completo\n",
    "# Ele combina o pré-processador (passo 'preprocessor')\n",
    "# com o modelo de classificação (passo 'classifier')\n",
    "model_lr_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # Nosso ColumnTransformer da Célula 12\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000)) \n",
    "    # max_iter=1000 ajuda o modelo a convergir com mais dados\n",
    "])\n",
    "\n",
    "# 3. Treinar o modelo!\n",
    "print(\"Iniciando o treinamento do modelo LogisticRegression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# O .fit() aqui dispara todo o processo (Scaler, OneHotEncoder E o treino)\n",
    "model_lr_pipe.fit(X_train, y_train) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Treinamento do LogisticRegression concluído em {(end_time - start_time):.2f} segundos.\")\n",
    "\n",
    "print(\"\\nModelo baseline (LogisticRegression) treinado com sucesso!\")\n",
    "display(model_lr_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 14: Avaliando o Modelo Baseline (LogisticRegression)\n",
    "\n",
    "# 1. Importações necessárias\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Avaliando o modelo LogisticRegression nos dados de TESTE...\")\n",
    "\n",
    "# 2. Fazer previsões nos dados de teste (X_test)\n",
    "# O .predict() aqui dispara todo o pipeline (Scaler, OneHotEncoder E a previsão)\n",
    "y_pred_lr = model_lr_pipe.predict(X_test)\n",
    "\n",
    "# 3. Gerar o Relatório de Classificação\n",
    "print(\"\\nRelatório de Classificação (LogisticRegression):\")\n",
    "# target_names dá nomes mais claros para '0' e '1'\n",
    "report_lr = classification_report(\n",
    "    y_test, \n",
    "    y_pred_lr, \n",
    "    target_names=['0 (Não Atrasado)', '1 (Atrasado)']\n",
    ")\n",
    "print(report_lr)\n",
    "\n",
    "# 4. Gerar a Matriz de Confusão\n",
    "print(\"Plotando Matriz de Confusão (LogisticRegression):\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Plota a matriz usando os valores reais (y_test) vs. os previstos (y_pred_lr)\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred_lr, \n",
    "    ax=ax, \n",
    "    cmap='Blues',\n",
    "    display_labels=['Não Atrasado', 'Atrasado']\n",
    ")\n",
    "plt.title('Matriz de Confusão - LogisticRegression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586296b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 15 (CORRIGIDA E COMPLETA): Treinar e Avaliar Modelo 2\n",
    "\n",
    "# --- Importações necessárias (resolvendo NameErrors) ---\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "# --------------------------------------------------------\n",
    "\n",
    "print(\"Iniciando o treinamento do Modelo 2 (LogisticRegression com class_weight='balanced')\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Criar um NOVO pipeline\n",
    "# (Certifique-se de que 'preprocessor', X_train, y_train existem das células 11 e 12)\n",
    "model_lr_balanced_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        random_state=42, \n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'\n",
    "    )) \n",
    "])\n",
    "\n",
    "# 2. Treinar o novo modelo\n",
    "model_lr_balanced_pipe.fit(X_train, y_train) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Treinamento concluído em {(end_time - start_time):.2f} segundos.\")\n",
    "\n",
    "# 3. Avaliar o novo modelo\n",
    "print(\"\\nAvaliando o modelo (LogisticRegression com class_weight='balanced') nos dados de TESTE...\")\n",
    "\n",
    "y_pred_lr_balanced = model_lr_balanced_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\nRelatório de Classificação (LR com class_weight):\")\n",
    "report_lr_balanced = classification_report(\n",
    "    y_test, \n",
    "    y_pred_lr_balanced, \n",
    "    target_names=['0 (Não Atrasado)', '1 (Atrasado)']\n",
    ")\n",
    "print(report_lr_balanced)\n",
    "\n",
    "# 4. Nova Matriz de Confusão\n",
    "print(\"Plotando Matriz de Confusão (LR com class_weight):\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred_lr_balanced, \n",
    "    ax=ax, \n",
    "    cmap='Blues',\n",
    "    display_labels=['Não Atrasado', 'Atrasado']\n",
    ")\n",
    "plt.title('Matriz de Confusão - LogisticRegression (class_weight=balanced)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 15.5: Verificando as Features do Modelo Treinado\n",
    "\n",
    "print(\"Inspecionando as features que o modelo (Célula 15) realmente usou:\")\n",
    "\n",
    "try:\n",
    "    # 1. Pega os nomes das features DEPOIS do ColumnTransformer\n",
    "    # (ex: 'num__HOUR_OF_DAY', 'cat__PART_OF_DAY_Manhã')\n",
    "    feature_names = model_lr_balanced_pipe.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    # 2. Pega os coeficientes (importância) do modelo LogReg\n",
    "    coefficients = model_lr_balanced_pipe.named_steps['classifier'].coef_[0]\n",
    "    \n",
    "    # 3. Cria um DataFrame para visualização fácil\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features que o modelo mais usou:\")\n",
    "    display(coef_df.head(10))\n",
    "    \n",
    "    print(\"\\nÚltimas 10 features que o modelo usou:\")\n",
    "    display(coef_df.tail(10))\n",
    "    \n",
    "    # 4. Verificação final\n",
    "    if 'num__SCHEDULED_DEPARTURE' in coef_df['Feature'].values:\n",
    "        print(\"\\n--- VEREDITO ---\")\n",
    "        print(\"FALHA NO PIPELINE: O modelo usou a feature ANTIGA ('SCHEDULED_DEPARTURE').\")\n",
    "        print(\"Causa provável: A Célula 11 ou 12 não foi re-executada antes da Célula 15.\")\n",
    "        \n",
    "    elif 'num__HOUR_OF_DAY' in coef_df['Feature'].values:\n",
    "        print(\"\\n--- VEREDITO ---\")\n",
    "        print(\"PIPELINE CORRETO: O modelo usou as features NOVAS ('HOUR_OF_DAY', 'PART_OF_DAY').\")\n",
    "        print(\"Conclusão: As novas features não melhoraram o desempenho do LogisticRegression.\")\n",
    "    else:\n",
    "        print(\"\\n--- VEREDITO ---\")\n",
    "        print(\"Verifique a lista de features manualmente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao inspecionar o modelo: {e}\")\n",
    "    print(\"Por favor, certifique-se de que as Células 11, 12 e 15 foram executadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77343be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 16: Preparando Dados para Clusterização (Não Supervisionado)\n",
    "\n",
    "# 1. Importações necessárias\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(\"Preparando dados para clusterização de aeroportos...\")\n",
    "\n",
    "# 2. Criar um DataFrame de 'features' por aeroporto\n",
    "# Vamos focar nos aeroportos de ORIGEM\n",
    "airport_features = df_flights_clean.groupby('ORIGIN_AIRPORT').agg(\n",
    "    \n",
    "    # Contagem total de voos saindo\n",
    "    total_flights=('FLIGHT_NUMBER', 'count'),\n",
    "    \n",
    "    # Média de atraso na SAÍDA\n",
    "    avg_departure_delay=('DEPARTURE_DELAY', 'mean'),\n",
    "    \n",
    "    # Porcentagem de voos que saíram atrasados (> 15 min)\n",
    "    pct_delayed_departures=('DEPARTURE_DELAY', lambda x: (x > 15).mean())\n",
    "    \n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# 3. Escalonar (Padronizar) os dados\n",
    "# K-Means é muito sensível à escala. Não podemos comparar 'total_flights' \n",
    "# (que vai a 100.000) com 'pct_delayed' (que vai de 0 a 1).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Salvamos os nomes dos aeroportos antes de escalonar\n",
    "airport_names = airport_features['ORIGIN_AIRPORT']\n",
    "\n",
    "# Removemos o nome para escalar apenas os números\n",
    "features_to_scale = airport_features.drop(columns=['ORIGIN_AIRPORT'])\n",
    "\n",
    "# Escalona\n",
    "features_scaled = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "print(\"\\nFeatures dos aeroportos prontas e escalonadas.\")\n",
    "print(f\"Total de aeroportos únicos: {len(airport_features)}\")\n",
    "display(airport_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 17: Método do Cotovelo (Elbow Method) para encontrar o K ideal\n",
    "\n",
    "if 'features_scaled' in locals():\n",
    "    print(\"Executando o Método do Cotovelo para K de 1 a 10...\")\n",
    "    \n",
    "    inertia_list = []  # Lista para guardar a inércia de cada K\n",
    "    k_range = range(1, 11) # Vamos testar K de 1 a 10\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Cria e treina o modelo K-Means para o 'k' atual\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k, \n",
    "            random_state=42, \n",
    "            n_init=10 # n_init=10 para suprimir warnings\n",
    "        )\n",
    "        kmeans.fit(features_scaled)\n",
    "        \n",
    "        # Adiciona a inércia (WCSS) à lista\n",
    "        inertia_list.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plotar o gráfico do cotovelo\n",
    "    print(\"Plotando o gráfico do Método do Cotovelo...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertia_list, marker='o', linestyle='--')\n",
    "    plt.xlabel('Número de Clusters (K)')\n",
    "    plt.ylabel('Inertia (WCSS)')\n",
    "    plt.title('Método do Cotovelo (Elbow Method) para K-Means')\n",
    "    plt.xticks(k_range)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Erro: 'features_scaled' não foi criado. Rode a Célula 16 primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 18: Rodando K-Means (K=4) e Analisando os Clusters\n",
    "\n",
    "if 'features_scaled' in locals() and 'airport_features' in locals():\n",
    "    \n",
    "    # 1. Definir o número de clusters com base no nosso \"Cotovelo\"\n",
    "    K_IDEAL = 4\n",
    "    \n",
    "    # 2. Rodar o modelo K-Means final\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=K_IDEAL, \n",
    "        random_state=42, \n",
    "        n_init=10\n",
    "    )\n",
    "    kmeans.fit(features_scaled)\n",
    "    \n",
    "    # 3. Adicionar os rótulos dos clusters de volta ao DataFrame original\n",
    "    airport_features_clustered = airport_features.copy()\n",
    "    airport_features_clustered['cluster'] = kmeans.labels_\n",
    "    \n",
    "    print(f\"K-Means executado com K={K_IDEAL}.\")\n",
    "    print(\"DataFrame com clusters:\")\n",
    "    display(airport_features_clustered.head())\n",
    "    \n",
    "    # 4. Interpretar os Clusters\n",
    "    # (Esta é a parte mais importante!)\n",
    "    # Vamos calcular a média de cada feature para cada cluster\n",
    "    print(\"\\n--- Interpretação dos Clusters (Médias) ---\")\n",
    "    cluster_interpretation = airport_features_clustered.groupby('cluster').agg({\n",
    "        'total_flights': 'mean',\n",
    "        'avg_departure_delay': 'mean',\n",
    "        'pct_delayed_departures': 'mean',\n",
    "        'ORIGIN_AIRPORT': 'count'  # Para ver quantos aeroportos em cada cluster\n",
    "    }).rename(columns={'ORIGIN_AIRPORT': 'airport_count'}).sort_values(by='total_flights')\n",
    "    \n",
    "    display(cluster_interpretation)\n",
    "    \n",
    "    # 5. Visualizar os Clusters (Requisito Obrigatório)\n",
    "    print(\"\\n--- Visualização dos Clusters ---\")\n",
    "    \n",
    "    # Vamos plotar Atraso Médio vs. % de Voos Atrasados\n",
    "    # O tamanho (size) do ponto será o Total de Voos\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(\n",
    "        data=airport_features_clustered,\n",
    "        x='avg_departure_delay',\n",
    "        y='pct_delayed_departures',\n",
    "        hue='cluster',         # Cor por cluster\n",
    "        size='total_flights',  # Tamanho por volume de voos\n",
    "        sizes=(50, 2000),      # Range dos tamanhos\n",
    "        palette='viridis',     # Esquema de cores\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Clusterização de Aeroportos por Perfil de Atraso (K={K_IDEAL})', fontsize=16)\n",
    "    plt.xlabel('Atraso Médio na Partida (min)')\n",
    "    plt.ylabel('Porcentagem de Voos Atrasados (>15 min)')\n",
    "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Erro: 'features_scaled' ou 'airport_features' não criados. Rode as células anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 19: Treinar o Modelo 3 (RandomForestClassifier em uma Amostra)\n",
    "\n",
    "# 1. Importações necessárias\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 2. Criar uma amostra de 10% dos dados de treino (para velocidade)\n",
    "# RandomForest é computacionalmente caro. Vamos usar 450k linhas.\n",
    "print(\"Criando amostra de 10% dos dados de treino (aprox. 450k linhas)...\")\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train, y_train, \n",
    "    train_size=0.1,  # 10% dos dados\n",
    "    random_state=42, \n",
    "    stratify=y_train # Mantém a proporção de 81/19 na amostra\n",
    ")\n",
    "\n",
    "print(f\"Tamanho da amostra de treino: {X_train_sample.shape}\")\n",
    "\n",
    "# 3. Criar o Pipeline completo\n",
    "print(\"Iniciando o treinamento do Modelo 3 (RandomForestClassifier)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        random_state=42, \n",
    "        class_weight='balanced', # Crucial para o desbalanceamento\n",
    "        n_jobs=-1                # Usar todos os processadores\n",
    "    )) \n",
    "])\n",
    "\n",
    "# 4. Treinar o modelo (NA AMOSTRA!)\n",
    "model_rf_pipe.fit(X_train_sample, y_train_sample) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Treinamento do RandomForest concluído em {(end_time - start_time):.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 20: Avaliando o Modelo 3 (RandomForestClassifier)\n",
    "\n",
    "print(\"\\nAvaliando o modelo (RandomForestClassifier) nos dados de TESTE COMPLETOS...\")\n",
    "\n",
    "# 1. Importações (caso o kernel tenha sido reiniciado)\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Fazer previsões (no X_test completo)\n",
    "# (Certifique-se que model_rf_pipe existe da Célula 19)\n",
    "y_pred_rf = model_rf_pipe.predict(X_test)\n",
    "\n",
    "# 3. Gerar o Relatório de Classificação\n",
    "print(\"\\nRelatório de Classificação (RandomForestClassifier):\")\n",
    "report_rf = classification_report(\n",
    "    y_test, \n",
    "    y_pred_rf, \n",
    "    target_names=['0 (Não Atrasado)', '1 (Atrasado)']\n",
    ")\n",
    "print(report_rf)\n",
    "\n",
    "# 4. Gerar a Matriz de Confusão\n",
    "print(\"Plotando Matriz de Confusão (RandomForestClassifier):\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_pred_rf, \n",
    "    ax=ax, \n",
    "    cmap='Blues',\n",
    "    display_labels=['Não Atrasado', 'Atrasado']\n",
    ")\n",
    "plt.title('Matriz de Confusão - RandomForestClassifier (Novas Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Célula 21: Importância das Features (RandomForest) ---\n",
    "import pandas as pd\n",
    "print(\"\\n--- Importância das Features (Feature Importances) ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Pega os nomes das features DEPOIS do ColumnTransformer\n",
    "    # (ex: 'num__HOUR_OF_DAY', 'cat__PART_OF_DAY_Manhã')\n",
    "    feature_names = model_rf_pipe.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    # 2. Pega as importâncias do classificador\n",
    "    importances = model_rf_pipe.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # 3. Cria um DataFrame para visualização fácil\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 features mais importantes para o RandomForest:\")\n",
    "    display(importance_df.head(15))\n",
    "    \n",
    "    # 4. Verificação final\n",
    "    if 'num__HOUR_OF_DAY' in importance_df['Feature'].head(5).values:\n",
    "         print(\"\\n--- VEREDITO ---\")\n",
    "         print(\"SUCESSO: O RandomForest usou suas features de engenharia (HOUR_OF_DAY)!\")\n",
    "    else:\n",
    "         print(\"\\n--- VEREDITO ---\")\n",
    "         print(\"Interessante... o RandomForest achou outras features mais importantes.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao extrair feature importance: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
